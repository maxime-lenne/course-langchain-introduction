# Introduction Ã  LangChain

![LangChain](img/langchain.jpeg)

**LangChain** est un framework open source conÃ§u pour construire des applications d'intelligence
artificielle autour des modÃ¨les de langage (LLMs) comme GPT, Claude ou Mistral. Il a la capacitÃ©
de se connecter aux LLMs, Ã  des sources de donnÃ©es, des outils, des chaÃ®nes de raisonnement et
des moyens de stockage pour crÃ©er des systÃ¨mes interactifs et dynamiques.

![LangChain components](img/langchain_components.png)

**LLM** â€” Les LLMs sont les moteurs de raisonnement, de gÃ©nÃ©ration de texte ou de prise de
dÃ©cision. LangChain les encapsule pour les intÃ©grer facilement dans des workflows intelligents.

**Prompts** â€” Les prompts sont la maniÃ¨re dont on guide un modÃ¨le. LangChain fournit des outils
pour construire des prompts dynamiques, rÃ©utilisables et paramÃ©trables.

**Chains** â€” Une `chain` est une sÃ©quence logique d'appels Ã  un LLM et Ã  d'autres composants
(par exemple : extraction d'information â†’ recherche vectorielle â†’ gÃ©nÃ©ration de rÃ©ponse).
Elle permet de crÃ©er des **pipelines IA personnalisÃ©s** pour des tÃ¢ches complexes.

**Memory** â€” LangChain permet de gÃ©rer une mÃ©moire conversationnelle, c'est-Ã -dire la capacitÃ©
Ã  se souvenir des Ã©changes passÃ©s. Cela rend les interactions plus naturelles et contextuelles
dans les agents ou les chatbots.

**Agents** â€” Les agents vont plus loin : ils choisissent dynamiquement les actions Ã  effectuer
Ã  partir d'outils disponibles (recherche web, calcul, consultation de base de donnÃ©esâ€¦).
Ils peuvent dÃ©cider quel outil appeler, avec quelles donnÃ©es, et enchaÃ®ner plusieurs Ã©tapes
de faÃ§on autonome.

**Documents Loader**, **Text Splitters**, **Indexes** et **Vector DB** â€” Ces composants forment
la chaÃ®ne d'ingestion de connaissances :

- le Documents Loader charge des documents bruts depuis des fichiers, APIs, bases de donnÃ©es
  ou sites web.
- les Text Splitters dÃ©coupent ces documents en chunks (morceaux de texte) pour respecter les
  limites de contexte des LLMs.
- le Vector DB : encode les chunks en vecteurs (via des embeddings) et les stocke dans une base
  vectorielle pour permettre une recherche par similaritÃ©.
- les Indexes centralisent et organisent ces composants pour structurer une base consultable.
  Ils permettent Ã  un agent ou une chaÃ®ne de retrouver les informations pertinentes pour une
  tÃ¢che donnÃ©e (Q/R, rÃ©sumÃ©, etc.).

## 1. Chargement du modÃ¨le LLM local

Dans cette section, nous chargeons un modÃ¨le de langage local grÃ¢ce Ã  **Ollama**. Cela permet
de travailler avec un **LLM directement sur notre machine**, sans connexion Ã  une API externe.

Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d'interagir
facilement avec un modÃ¨le comme llama3 dÃ©jÃ  tÃ©lÃ©chargÃ© via Ollama.

## 2. RequÃªte basique

Maintenant que notre modÃ¨le est chargÃ©, nous pouvons lui envoyer une premiÃ¨re requÃªte simple.
Ici, nous utilisons la mÃ©thode `.invoke()` pour poser une question directe.

Cela nous permet de tester rapidement le bon fonctionnement du modÃ¨le et d'observer comment
il formule ses rÃ©ponses.

### ğŸ§© Exercices

#### Exercice 1

Utilisez le modÃ¨le pour transformer une phrase simple en la rÃ©Ã©crivant dans un style littÃ©raire
spÃ©cifique.

1. Envoyez une requÃªte directe (sans PromptTemplate) via .invoke() contenant :

- une instruction claire au modÃ¨le,
- une phrase source,
- le style souhaitÃ© (ex. : Shakespeare, roman noir, science-fiction, etc.).

1. Affichez uniquement le rÃ©sultat retournÃ© par le LLM.

#### Exercice 2

Tu es en mission pour rÃ©diger un message diplomatique adressÃ© Ã  une civilisation extraterrestre
trÃ¨s susceptible.

1. Envoyez une requÃªte au modÃ¨le via .invoke() avec un prompt complet :

- contexte fictif : situation tendue,
- contraintes : Ã©viter certains mots, rester poli,
- objectif : obtenir la paix ou proposer une alliance.

1. Observez comment le modÃ¨le gÃ¨re le ton et les instructions.

## 3. Conversations avec le modÃ¨le

MÃªme si tout mettre dans un seul message peut fonctionner dans des cas simples, des types de
messages diffÃ©rent nous donne plus de contrÃ´le sur le dialogue et permet de mieux exploiter les
capacitÃ©s du modÃ¨le, surtout dans des systÃ¨mes plus complexes comme des agents ou des chatbots.

C'est pour cela que, plutÃ´t que de tout Ã©crire dans une seule phrase, il est recommandÃ© de
distinguer diffÃ©rents types de messages :

- `SystemMessage` : permet de dÃ©finir le rÃ´le ou le comportement attendu du modÃ¨le
  (par exemple : "Vous Ãªtes un assistant qui rÃ©pond en franÃ§ais").
- `HumanMessage` : correspond Ã  ce que vous demandez rÃ©ellement au modÃ¨le.
- `AIMessage` : reprÃ©sente une rÃ©ponse prÃ©cÃ©dente du modÃ¨le, utile si nous construisons
  une conversation continue.

### 3.1 Conversation sans mÃ©moire (stateless)

Dans l'exemple suivant, nous structurons notre requÃªte en simulant une interaction avec le modÃ¨le.
Nous sÃ©parons le contexte gÃ©nÃ©ral (via un SystemMessage) de la question posÃ©e (via un HumanMessage).

### 3.2 Conversation avec mÃ©moire (stateful)

Dans l'exemple qui suit, nous simulons une conversation Ã  plusieurs tours avec le modÃ¨le.
Nous utilisons un AIMessage pour rappeler la rÃ©ponse prÃ©cÃ©dente, ce qui permet au modÃ¨le de
garder le fil du dialogue et de rÃ©pondre naturellement Ã  une nouvelle question en lien avec
la prÃ©cÃ©dente.

Dans le second exemple ci-dessous, nous mettons en place une boucle de conversation interactive
avec le modÃ¨le.
Ã€ chaque Ã©change, la question de l'utilisateur et la rÃ©ponse du modÃ¨le sont ajoutÃ©es Ã 
l'historique (`chat_history`).
Cela permet au LLM de garder en mÃ©moire le contexte et de rÃ©pondre de faÃ§on plus cohÃ©rente
tout au long de la discussion.

### ğŸ§© Exercices

> Exercice 1

1. CrÃ©ez une liste messages avec :

- un SystemMessage qui indique que l'IA est un expert dans un domaine de ton choix
  (maths, histoire, cinÃ©ma, etc.),
- un HumanMessage qui pose une question Ã  l'IA.

1. Envoyez cette liste Ã  model.invoke(messages) et affiche la rÃ©ponse.

> Exercice 2

CrÃ©er une mini-conversation avec l'IA, oÃ¹ chaque question/rÃ©ponse est ajoutÃ©e Ã  l'historique
des messages. L'IA doit se souvenir de l'Ã©change prÃ©cÃ©dent.

1. Initialisez une liste messages avec un SystemMessage dÃ©finissant le rÃ´le de l'IA.
2. Dans une boucle :

- Demandez une question Ã  l'utilisateur (input()),
- Ajoutez un HumanMessage Ã  la liste,
- Envoyez la liste complÃ¨te Ã  model.invoke(...),
- Affichez la rÃ©ponse de l'IA,
- Ajoutez cette rÃ©ponse comme AIMessage Ã  la liste.

1. ArrÃªtez la boucle si l'utilisateur entre "stop".

## 4. Conversations avec le modÃ¨le Ã  l'aide de Prompt Templates

Nous allons explorer l'utilisation de `ChatPromptTemplate`, un outil qui permet de structurer
proprement les messages envoyÃ©s Ã  un modÃ¨le de type "chat" (comme GPT-4).

`ChatPromptTemplate` permet de construire une conversation multi-rÃ´le en distinguant les messages
systÃ¨me (rÃ¨gles, rÃ´le de l'IA), humains (questions ou commandes) et les rÃ©ponses de l'IA.

### 4.1 Prompt conversation Ã  rÃ´le unique (human)

Ce type de prompt utilise la fonction `.from_template( )` et est de type `human` par dÃ©faut,
c'est un prompte simple "tout en un" oÃ¹ il n'est pas possible de contrÃ´ler le rÃ´le.

### 4.2 Prompt conversation Ã  rÃ´les multiples (system, assistant, human)

Ce type de prompt utilise la fonction `.from_messages( )` et permet de dÃ©finir **plusieurs
messages avec des rÃ´les explicites** (system, human, etc.).
C'est un prompt structurÃ©, idÃ©al pour guider prÃ©cisÃ©ment le comportement du modÃ¨le dans un
contexte conversationnel.

### ğŸ§© Exercices

> Exercice 1

Construire un assistant capable d'adopter le style d'un philosophe cÃ©lÃ¨bre pour rÃ©pondre Ã  des
questions existentielles.

1. CrÃ©ez un ChatPromptTemplate avec :

- un message system dÃ©finissant l'IA comme un philosophe prÃ©cis ({philosopher}),
- un message human contenant une question {question}.

1. Injectez des variables avec :

- un nom de philosophe (ex. : Socrate, Nietzsche, Simone de Beauvoir),
- une question philosophique.

1. Affichez la rÃ©ponse du modÃ¨le, en observant si le style correspond au philosophe choisi.

> Exercice 2

Simulez une conversation entre un utilisateur et un LLM autour d'un sujet (ex. : mathÃ©matiques,
littÃ©rature, programmation) en construisant dynamiquement le prompt avec `ChatPromptTemplate`.

ImplÃ©mentez une boucle qui :

- Initialise un prompt avec un message system.

Ã€ chaque tour :

- Prend une entrÃ©e utilisateur (input()),
- Ajoute un message human,
- Envoie le tout au LLM,
- Affiche la rÃ©ponse,
- Ajoute un message de type/role `assistant` contenant la rÃ©ponse (ğŸ’¡ ce rÃ´le correspond Ã 
  la rÃ©ponse de l'IA et est l'Ã©quivalent de AIMessage).
- ArrÃªte la conversation si l'utilisateur entre "stop".
