{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f4459b",
   "metadata": {},
   "source": [
    "![LangChain](img/langchain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150e78f",
   "metadata": {},
   "source": [
    "**LangChain** est un framework open source con√ßu pour construire des applications d‚Äôintelligence artificielle autour des mod√®les de langage (LLMs) comme GPT, Claude ou Mistral. Il a la capacit√© de se connecter aux LLMs, √† des sources de donn√©es, des outils, des cha√Ænes de raisonnement et des moyens de stockage pour cr√©er des syst√®mes interactifs et dynamiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6f6be",
   "metadata": {},
   "source": [
    "![LangChain components](img/langchain_components.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c20df",
   "metadata": {},
   "source": [
    "> **LLM**\n",
    "--\n",
    "Les LLMs sont les moteurs de raisonnement, de g√©n√©ration de texte ou de prise de d√©cision. LangChain les encapsule pour les int√©grer facilement dans des workflows intelligents.\n",
    "\n",
    "> **Prompts**\n",
    "--\n",
    "Les prompts sont la mani√®re dont on guide un mod√®le. LangChain fournit des outils pour construire des prompts dynamiques, r√©utilisables et param√©trables.\n",
    "\n",
    "> **Chains**\n",
    "--\n",
    "Une `chain` est une s√©quence logique d‚Äôappels √† un LLM et √† d‚Äôautres composants (par exemple : extraction d‚Äôinformation ‚Üí recherche vectorielle ‚Üí g√©n√©ration de r√©ponse). Elle permet de cr√©er des **pipelines IA personnalis√©s** pour des t√¢ches complexes.\n",
    "\n",
    "> **Memory**\n",
    "--\n",
    "LangChain permet de g√©rer une m√©moire conversationnelle, c‚Äôest-√†-dire la capacit√© √† se souvenir des √©changes pass√©s. Cela rend les interactions plus naturelles et contextuelles dans les agents ou les chatbots.\n",
    "\n",
    "> **Agents**\n",
    "--\n",
    "Les agents vont plus loin : ils choisissent dynamiquement les actions √† effectuer √† partir d‚Äôoutils disponibles (recherche web, calcul, consultation de base de donn√©es‚Ä¶). Ils peuvent d√©cider quel outil appeler, avec quelles donn√©es, et encha√Æner plusieurs √©tapes de fa√ßon autonome.\n",
    "\n",
    "> **Documents Loader**, **Text Splitters**, **Indexes** et **Vector DB**\n",
    "--\n",
    "> Ces composants forment la cha√Æne d‚Äôingestion de connaissances :\n",
    "> - le Documents Loader charge des documents bruts depuis des fichiers, APIs, bases de donn√©es ou sites web.\n",
    "> - les Text Splitters d√©coupent ces documents en chunks (morceaux de texte) pour respecter les limites de contexte des LLMs.\n",
    "> - le Vector DB : encode les chunks en vecteurs (via des embeddings) et les stocke dans une base vectorielle pour permettre une recherche par similarit√©.\n",
    "> - les Indexes centralisent et organisent ces composants pour structurer une base consultable. Ils permettent √† un agent ou une cha√Æne de retrouver les informations pertinentes pour une t√¢che donn√©e (Q/R, r√©sum√©, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7eb01a",
   "metadata": {},
   "source": [
    "# 1. Chargement du mod√®le\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafb689",
   "metadata": {},
   "source": [
    "## LLM local :\n",
    "\n",
    "Dans cette section, nous chargeons un mod√®le de langage local gr√¢ce √† **Ollama**. Cela permet de travailler avec un **LLM directement sur notre machine**, sans connexion √† une API externe.\n",
    "\n",
    "Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d‚Äôinteragir facilement avec un mod√®le comme llama3 d√©j√† t√©l√©charg√© via Ollama.\n",
    "\n",
    "GTP-OSS:20b, Mistral-Small3.2 24B GLM 4.7 Flash\n",
    "\n",
    "## LLM Cloud :\n",
    "Mistral"
   ]
  },
  {
   "cell_type": "code",
   "id": "7301c899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:46:05.657666Z",
     "start_time": "2026-02-09T12:46:05.110517Z"
    }
   },
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# Chargement des cl√©s d'API se trouvant dans le fichier .env.\n",
    "# Ceci permet d'utiliser des mod√®les en ligne comme gpt-x, deepseek-x, etc...\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model = ChatOllama(model=\"glm-4.7-flash\")\n",
    "\n",
    "#model = ChatMistralAI(model=\"mistral-large-latest\", api_key=os.getenv(\"MISTRAL_API_KEY\"))"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "a48a0cd4",
   "metadata": {},
   "source": [
    "# 2. Requ√™te basique\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc2daf",
   "metadata": {},
   "source": [
    "Maintenant que notre mod√®le est charg√©, nous pouvons lui envoyer une premi√®re requ√™te simple. Ici, nous utilisons la m√©thode `.invoke()` pour poser une question directe.\n",
    "\n",
    "Cela nous permet de tester rapidement le bon fonctionnement du mod√®le et d‚Äôobserver comment il formule ses r√©ponses."
   ]
  },
  {
   "cell_type": "code",
   "id": "da43aef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:46:18.567304Z",
     "start_time": "2026-02-09T12:46:05.659654Z"
    }
   },
   "source": [
    "# Envoie une requ√™te simple au mod√®le LLM via la m√©thode `invoke`\n",
    "# Ici, on pose un probl√®me de math√©matiques en langage naturel\n",
    "result = model.invoke(\"R√©sous ce probl√®me de math√©matiques. Quel est le r√©sultat de la division de 4 par 2 ?\")\n",
    "\n",
    "# Affiche uniquement la r√©ponse g√©n√©r√©e par le mod√®le (sans m√©tadonn√©es)\n",
    "display(Markdown(result.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Le r√©sultat est **2**.\n\nEn math√©matiques, $4 \\div 2 = 2$."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "907ad906",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523de175",
   "metadata": {},
   "source": [
    "#### Exercice 1\n",
    "\n",
    "Utilisez le mod√®le pour transformer une phrase simple en la r√©√©crivant dans un style litt√©raire sp√©cifique.\n",
    "1.\tEnvoyez une requ√™te directe (sans PromptTemplate) via .invoke() contenant :\n",
    "- une instruction claire au mod√®le,\n",
    "- une phrase source,\n",
    "- le style souhait√© (ex. : Shakespeare, roman noir, science-fiction, etc.).\n",
    "2.\tAffichez uniquement le r√©sultat retourn√© par le LLM."
   ]
  },
  {
   "cell_type": "code",
   "id": "cc65bc41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:46:18.618169Z",
     "start_time": "2026-02-09T12:46:18.612015Z"
    }
   },
   "source": "# Votre code ici",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "29e523ab",
   "metadata": {},
   "source": [
    "#### Exercice 2\n",
    "\n",
    "Tu es en mission pour r√©diger un message diplomatique adress√© √† une civilisation extraterrestre tr√®s susceptible.\n",
    "1.\tEnvoyez une requ√™te au mod√®le via .invoke() avec un prompt complet :\n",
    "- contexte fictif : situation tendue,\n",
    "- contraintes : √©viter certains mots, rester poli,\n",
    "- objectif : obtenir la paix ou proposer une alliance.\n",
    "2.\tObservez comment le mod√®le g√®re le ton et les instructions."
   ]
  },
  {
   "cell_type": "code",
   "id": "93c105f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:46:18.652285Z",
     "start_time": "2026-02-09T12:46:18.623002Z"
    }
   },
   "source": [
    "# Votre code ici"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "55d3e0b3",
   "metadata": {},
   "source": [
    "# 3. Conversations avec le mod√®le\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ad6e1",
   "metadata": {},
   "source": [
    "M√™me si tout mettre dans un seul message peut fonctionner dans des cas simples, des types de messages diff√©rent nous donne plus de contr√¥le sur le dialogue et permet de mieux exploiter les capacit√©s du mod√®le, surtout dans des syst√®mes plus complexes comme des agents ou des chatbots.\n",
    "\n",
    "C'est pour cela que, plut√¥t que de tout √©crire dans une seule phrase, il est recommand√© de distinguer diff√©rents types de messages :\n",
    "- `SystemMessage` : permet de d√©finir le r√¥le ou le comportement attendu du mod√®le (par exemple : ‚ÄúVous √™tes un assistant qui r√©pond en fran√ßais‚Äù).\n",
    "- `HumanMessage` : correspond √† ce que vous demandez r√©ellement au mod√®le.\n",
    "- `AIMessage` : repr√©sente une r√©ponse pr√©c√©dente du mod√®le, utile si nous construisons une conversation continue."
   ]
  },
  {
   "cell_type": "code",
   "id": "000a40ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:46:18.659458Z",
     "start_time": "2026-02-09T12:46:18.655045Z"
    }
   },
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "19b8cd83",
   "metadata": {},
   "source": [
    "### 3.1 Conversation sans m√©moire (stateless)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87d83e",
   "metadata": {},
   "source": [
    "Dans l'exemple suivant, nous structurons notre requ√™te en simulant une interaction avec le mod√®le.\n",
    "Nous s√©parons le contexte g√©n√©ral (via un SystemMessage) de la question pos√©e (via un HumanMessage)."
   ]
  },
  {
   "cell_type": "code",
   "id": "27747fe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:46:23.566650Z",
     "start_time": "2026-02-09T12:46:18.660299Z"
    }
   },
   "source": [
    "# On d√©finit une liste de messages structur√©s pour guider le comportement du mod√®le\n",
    "messages = [\n",
    "    # Le SystemMessage pr√©cise le r√¥le ou l'objectif g√©n√©ral : ici, r√©soudre un probl√®me math√©matique\n",
    "    SystemMessage(content=\"R√©sous ce probl√®me de math√©matiques\"),\n",
    "\n",
    "    # Le HumanMessage contient la question concr√®te pos√©e par l'utilisateur\n",
    "    HumanMessage(content=\"Quel est le r√©sultat de la division de 4 par 2 ?\")\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "display(Markdown(result.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Le r√©sultat de la division de 4 par 2 est **2**."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "f9d963ae",
   "metadata": {},
   "source": [
    "### 3.2 Conversation avec m√©moire (stateful)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0e640",
   "metadata": {},
   "source": [
    "Dans l'exemple qui suit, nous simulons une conversation √† plusieurs tours avec le mod√®le.\n",
    "Nous utilisons un AIMessage pour rappeler la r√©ponse pr√©c√©dente, ce qui permet au mod√®le de garder le fil du dialogue et de r√©pondre naturellement √† une nouvelle question en lien avec la pr√©c√©dente."
   ]
  },
  {
   "cell_type": "code",
   "id": "d8e4ae45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:46:26.350771Z",
     "start_time": "2026-02-09T12:46:23.574565Z"
    }
   },
   "source": [
    "# On construit une liste de messages simulant une conversation en plusieurs √©tapes\n",
    "messages = [\n",
    "    # Le SystemMessage d√©finit le r√¥le g√©n√©ral du mod√®le : ici, r√©soudre des probl√®mes de math√©matiques\n",
    "    SystemMessage(content=\"R√©sous ce probl√®me de math√©matiques\"),\n",
    "\n",
    "    # Premier message de l'utilisateur : une question simple\n",
    "    HumanMessage(content=\"Quel est le r√©sultat de la division de 4 par 2 ?\"),\n",
    "\n",
    "    # R√©ponse simul√©e du mod√®le √† la premi√®re question (permet de maintenir le contexte)\n",
    "    AIMessage(content=\"Le r√©sultat de la division de 4 par 2 est √©gal √† 2.\"),\n",
    "\n",
    "    # Deuxi√®me question de l'utilisateur, li√©e √† la pr√©c√©dente\n",
    "    HumanMessage(content=\"Et 8 multipli√© par 4 ?\"),\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "display(Markdown(result.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Le r√©sultat de $8$ multipli√© par $4$ est √©gal √† $32$."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "e4def04b",
   "metadata": {},
   "source": [
    "Dans le second exemple ci-dessous, nous mettons en place une boucle de conversation interactive avec le mod√®le.\n",
    "√Ä chaque √©change, la question de l‚Äôutilisateur et la r√©ponse du mod√®le sont ajout√©es √† l‚Äôhistorique (`chat_history`).\n",
    "Cela permet au LLM de garder en m√©moire le contexte et de r√©pondre de fa√ßon plus coh√©rente tout au long de la discussion."
   ]
  },
  {
   "cell_type": "code",
   "id": "3f50962e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:46:47.911973Z",
     "start_time": "2026-02-09T12:46:26.358652Z"
    }
   },
   "source": [
    "# Initialisation de l'historique des messages\n",
    "chat_history = []\n",
    "\n",
    "# Message syst√®me : donne un r√¥le au mod√®le pour toute la session\n",
    "system_message = SystemMessage(content=\"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\")\n",
    "chat_history.append(system_message)\n",
    "\n",
    "\n",
    "# Boucle principale de conversation (s'arr√™te si l'utilisateur tape 'exit')\n",
    "# ‚ö†Ô∏è `while Flase: ` √† modier en `while True: ` et invers√©ment lorsque vous souhaitez d√©sactiver ou activer cet exemple\n",
    "while True:\n",
    "    query = input(\"Vous : \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break  # Sortie de la boucle\n",
    "\n",
    "    # Ajout de la question de l'utilisateur dans l'historique\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "\n",
    "    # Envoi de tout l'historique au mod√®le pour maintenir le contexte\n",
    "    result = model.invoke(chat_history)\n",
    "    response = result.content\n",
    "\n",
    "    # Ajout de la r√©ponse du mod√®le dans l'historique\n",
    "    chat_history.append(AIMessage(content=response))\n",
    "\n",
    "    # Affichage de la r√©ponse √† l'utilisateur\n",
    "    print(response)\n",
    "\n",
    "\n",
    "print(\"------ Historique des messages ------\")\n",
    "print(chat_history)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le r√©sultat est **11**.\n",
      "\n",
      "Voici une petite explication pour visualiser l'op√©ration :\n",
      "\n",
      "1.  Si tu mets tes 5 doigts d'une main et que tu en ajoutes 6 autres, cela fait 11 doigts au total.\n",
      "2.  Tu peux aussi le voir comme : $5 + 5 = 10$, et il te reste encore $1$ √† ajouter.\n",
      "------ Historique des messages ------\n",
      "[SystemMessage(content='Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.', additional_kwargs={}, response_metadata={}), HumanMessage(content='5+6', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Le r√©sultat est **11**.\\n\\nVoici une petite explication pour visualiser l'op√©ration :\\n\\n1.  Si tu mets tes 5 doigts d'une main et que tu en ajoutes 6 autres, cela fait 11 doigts au total.\\n2.  Tu peux aussi le voir comme : $5 + 5 = 10$, et il te reste encore $1$ √† ajouter.\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "8b4cc395",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3306d3d",
   "metadata": {},
   "source": [
    "#### Exercice 3\n",
    "\n",
    "1.\tCr√©ez une liste messages avec :\n",
    "- un SystemMessage qui indique que l‚ÄôIA est un expert dans un domaine de ton choix (maths, histoire, cin√©ma, etc.),\n",
    "- un HumanMessage qui pose une question √† l‚ÄôIA.\n",
    "2.\tEnvoyez cette liste √† model.invoke(messages) et affiche la r√©ponse."
   ]
  },
  {
   "cell_type": "code",
   "id": "55850e3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:46:47.924533Z",
     "start_time": "2026-02-09T12:46:47.919810Z"
    }
   },
   "source": [
    "# Votre code ici"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "f136b7b2",
   "metadata": {},
   "source": [
    "#### Exercice 4\n",
    "\n",
    "Cr√©er une mini-conversation avec l‚ÄôIA, o√π chaque question/r√©ponse est ajout√©e √† l‚Äôhistorique des messages. L‚ÄôIA doit se souvenir de l‚Äô√©change pr√©c√©dent.\n",
    "\n",
    "1.\tInitialisez une liste messages avec un SystemMessage d√©finissant le r√¥le de l‚ÄôIA.\n",
    "2.\tDans une boucle :\n",
    "- Demandez une question √† l‚Äôutilisateur (input()),\n",
    "- Ajoutez un HumanMessage √† la liste,\n",
    "- Envoyez la liste compl√®te √† model.invoke(...),\n",
    "- Affichez la r√©ponse de l‚ÄôIA,\n",
    "- Ajoutez cette r√©ponse comme AIMessage √† la liste.\n",
    "3.\tArr√™tez la boucle si l‚Äôutilisateur entre \"stop\"."
   ]
  },
  {
   "cell_type": "code",
   "id": "4e745ff2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:46:47.939472Z",
     "start_time": "2026-02-09T12:46:47.925117Z"
    }
   },
   "source": [
    "# Votre code ici"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "99079f69",
   "metadata": {},
   "source": [
    "# 4. Conversations avec le mod√®le √† l'aide de Prompt Templates\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b858f",
   "metadata": {},
   "source": [
    "Nous allons explorer l‚Äôutilisation de `ChatPromptTemplate`, un outil qui permet de structurer proprement les messages envoy√©s √† un mod√®le de type ‚Äúchat‚Äù (comme GPT-4).\n",
    "\n",
    "`ChatPromptTemplate` permet de construire une conversation multi-r√¥le en distinguant les messages syst√®me (r√®gles, r√¥le de l‚ÄôIA), humains (questions ou commandes) et les r√©ponses de l‚ÄôIA."
   ]
  },
  {
   "cell_type": "code",
   "id": "367d951e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:46:47.970957Z",
     "start_time": "2026-02-09T12:46:47.940348Z"
    }
   },
   "source": "from langchain_core.prompts import ChatPromptTemplate",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "88b05d41",
   "metadata": {},
   "source": [
    "### 4.1 Prompt conversation √† r√¥le unique (human)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91fa28",
   "metadata": {},
   "source": [
    "Ce type de prompt utilise la fonction `.from_template( )` et est de type `human` par d√©faut, c'est un prompte simple \"tout en un\" o√π il n'est pas possible de contr√¥ler le r√¥le."
   ]
  },
  {
   "cell_type": "code",
   "id": "c6b3b1a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:47:06.304480Z",
     "start_time": "2026-02-09T12:46:47.971686Z"
    }
   },
   "source": [
    "# D√©finition d'un template simple (texte brut avec variables), sans r√¥les explicites\n",
    "template = \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine. Calcule le double de {value_1}, puis celui de {value_2}\"\n",
    "\n",
    "# Cr√©ation du prompt √† partir du template ; ce sera un message unique de type 'human' par d√©faut\n",
    "chat_prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Injection des valeurs dans les variables du template\n",
    "prompt_value = chat_prompt_template.invoke({\"value_1\": 12, \"value_2\": 34})\n",
    "\n",
    "# Envoi du prompt au mod√®le pour obtenir une r√©ponse\n",
    "result = model.invoke(prompt_value)\n",
    "\n",
    "display(Markdown(result.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Bonjour ! C'est un excellent exercice pour s'entra√Æner avec les multiplications.\n\nPour trouver le double d'un nombre, on le multiplie simplement par 2 (ou on l'ajoute √† lui-m√™me).\n\nVoici les calculs √©tape par √©tape :\n\n1.  **Le double de 12 :**\n    $12 \\times 2 = \\mathbf{24}$\n\n2.  **Le double de 34 :**\n    $34 \\times 2 = \\mathbf{68}$"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "404f0120",
   "metadata": {},
   "source": [
    "### 4.2 Prompt conversation √† r√¥les multiples (system, assistant, human)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ea67f",
   "metadata": {},
   "source": [
    "Ce type de prompt utilise la fonction `.from_messages( )` et permet de d√©finir **plusieurs messages avec des r√¥les explicites** (system, human, etc.).\n",
    "C‚Äôest un prompt structur√©, id√©al pour guider pr√©cis√©ment le comportement du mod√®le dans un contexte conversationnel."
   ]
  },
  {
   "cell_type": "code",
   "id": "8ce68b65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:47:23.932804Z",
     "start_time": "2026-02-09T12:47:06.318179Z"
    }
   },
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    # Message syst√®me : d√©finit le r√¥le et le comportement global du mod√®le\n",
    "    (\"system\", \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\"),\n",
    "    # Message utilisateur : pose une question contenant deux variables\n",
    "    (\"human\", \"Calcule le double de {value_1}, puis celui de {value_2}\")\n",
    "])\n",
    "\n",
    "# Injection des valeurs dans les variables du prompt\n",
    "prompt_value = chat_prompt_template.invoke({\"value_1\": 12, \"value_2\": 34})\n",
    "\n",
    "# Envoi du prompt structur√© au mod√®le pour obtenir une r√©ponse\n",
    "result = model.invoke(prompt_value)\n",
    "\n",
    "display(Markdown(result.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Voici les calculs √©tape par √©tape :\n\n1.  **Le double de 12 :**\n    Pour trouver le double d'un nombre, il faut le multiplier par 2 (ou l'ajouter √† lui-m√™me).\n    $$12 \\times 2 = 24$$\n\n2.  **Le double de 34 :**\n    De la m√™me mani√®re, nous multiplions 34 par 2.\n    $$34 \\times 2 = 68$$\n\n**R√©sultats :**\nLe double de 12 est **24**.\nLe double de 34 est **68**."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "1ddb6d7e",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2a134",
   "metadata": {},
   "source": [
    "#### Exercice 5\n",
    "\n",
    "Construire un assistant capable d‚Äôadopter le style d‚Äôun philosophe c√©l√®bre pour r√©pondre √† des questions existentielles.\n",
    "1.\tCr√©ez un ChatPromptTemplate avec :\n",
    "- un message system d√©finissant l‚ÄôIA comme un philosophe pr√©cis ({philosopher}),\n",
    "- un message human contenant une question {question}.\n",
    "2.\tInjectez des variables avec :\n",
    "- un nom de philosophe (ex. : Socrate, Nietzsche, Simone de Beauvoir),\n",
    "- une question philosophique.\n",
    "3.\tAffichez la r√©ponse du mod√®le, en observant si le style correspond au philosophe choisi."
   ]
  },
  {
   "cell_type": "code",
   "id": "ca1c929e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:47:23.951890Z",
     "start_time": "2026-02-09T12:47:23.946670Z"
    }
   },
   "source": [
    "# Votre code ici"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "9dfa107a",
   "metadata": {},
   "source": [
    "#### Exercice 6\n",
    "\n",
    "Simulez une conversation entre un utilisateur et un LLM autour d‚Äôun sujet (ex. : math√©matiques, litt√©rature, programmation) en construisant dynamiquement le prompt avec `ChatPromptTemplate`.\n",
    "\n",
    "Impl√©mentez une boucle qui :\n",
    "- Initialise un prompt avec un message system.\n",
    "\n",
    "√Ä chaque tour :\n",
    "- Prend une entr√©e utilisateur (input()),\n",
    "- Ajoute un message human,\n",
    "- Envoie le tout au LLM,\n",
    "- Affiche la r√©ponse,\n",
    "- Ajoute un message de type/role `assistant` contenant la r√©ponse (üí° ce r√¥le correspond √† la r√©ponse de l‚ÄôIA et est l'√©quivalent de AIMessage).\n",
    "- Arr√™te la conversation si l‚Äôutilisateur entre ‚Äústop‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "id": "b5d8dcb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:47:23.957253Z",
     "start_time": "2026-02-09T12:47:23.952646Z"
    }
   },
   "source": [
    "# Votre code ici"
   ],
   "outputs": [],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
